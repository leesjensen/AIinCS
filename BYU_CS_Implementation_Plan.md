# Implementation Plan: Claude AI Integration at BYU CS Department

**Target Launch:** Fall 2026 (August 2026)
**Target Courses:** CS 260 (Web Programming), CS 340 (Software Design), CS 329 (QA and DevOps)
**Prepared:** January 2026

## Executive Summary

This implementation plan outlines the timeline and key milestones for integrating Claude AI into three BYU Computer Science courses by Fall 2026. The plan emphasizes faculty preparation, autograder enhancement, clear student policies, and measurable outcomes.

**Critical Success Factors:**
1. Faculty buy-in and training completed by August 2026
2. Autograders enhanced to assess understanding, not just correctness
3. Clear AI usage policies established and communicated to students
4. Budget allocated for Claude licenses and faculty development
5. Assessment framework ready to measure learning outcomes

## Timeline Overview

```
Jan-Apr 2026: Planning & Preparation
May-Aug 2026: Faculty Training & Course Redesign
Aug-Dec 2026: Fall Semester Launch & Support
Dec 2026:     Assessment & Planning for Expansion
```

## Phase 1: Planning & Preparation (January - April 2026)

**Objective:** Establish foundation, secure resources, and align stakeholders

### January 2026
- **Milestone:** Department approval of recommendation and implementation plan
- **Action:** Present recommendation to department leadership and faculty
- **Action:** Identify faculty leads for CS 260, CS 340, CS 329
- **Action:** Form AI Integration Working Group (faculty + instructional design + TA coordinator)
- **Deliverable:** Approved plan with identified course leads

### February 2026
- **Milestone:** Licenses and technical infrastructure secured
- **Action:** Procure Claude Team/Enterprise licenses for students (estimated 200-300 seats)
- **Action:** Determine access model (department licenses vs. student subscriptions)
- **Action:** Set up administrative access for faculty and TAs
- **Action:** Coordinate with BYU IT on account provisioning
- **Deliverable:** Claude access ready for faculty and pilot students

### March 2026
- **Milestone:** AI Usage Policy Framework approved
- **Action:** Draft AI usage policy for target courses (3-tier framework: encouraged/allowed/prohibited)
- **Action:** Work with honor code office on academic integrity guidelines
- **Action:** Create template syllabi language for AI tool policies
- **Action:** Review and align with any university-wide AI policies
- **Deliverable:** Approved AI usage policy document for courses

### April 2026
- **Milestone:** Autograding enhancement plan finalized
- **Action:** Audit existing autograders for CS 260, CS 340, CS 329
- **Action:** Identify which assignments need redesign for multi-dimensional assessment
- **Action:** Develop specifications for enhanced autograders (design quality, explanation, testing)
- **Action:** Determine if Claude API will be used for automated explanation assessment
- **Action:** Allocate development resources (staff, TAs, or external contractors)
- **Deliverable:** Autograder enhancement roadmap with assigned owners

## Phase 2: Faculty Training & Course Redesign (May - August 2026)

**Objective:** Prepare faculty and courses for successful AI integration

### May 2026
- **Milestone:** Faculty workshop series begins
- **Action:** Workshop 1 - "Effective Use of Claude for Teaching" (hands-on with Claude capabilities)
- **Action:** Workshop 2 - "Designing AI-Era Assignments" (autograding for understanding, constraint-based assessment)
- **Action:** Workshop 3 - "Academic Integrity in the AI Era" (honor code integration, detecting vs. embracing AI use)
- **Action:** Provide course redesign stipends or release time for participating faculty
- **Deliverable:** Faculty trained on Claude pedagogy and assessment strategies

### June 2026
- **Milestone:** Course materials redesigned
- **Action:** CS 260 instructor redesigns assignments with AI-appropriate usage tiers
- **Action:** CS 340 instructor updates design projects with explanation requirements
- **Action:** CS 329 instructor redesigns testing/DevOps assignments for autograding
- **Action:** Develop shared assignment templates and rubrics for AI-era assessment
- **Action:** Create student-facing "How to Use Claude Effectively" guide
- **Deliverable:** Redesigned syllabi and assignments for all three courses

### July 2026
- **Milestone:** Autograders enhanced and tested
- **Action:** Implement multi-dimensional autograding (correctness + design + testing + explanation)
- **Action:** Integrate constraint-based checks (architecture patterns, performance requirements)
- **Action:** Build or integrate LLM-based explanation quality assessment
- **Action:** Test enhanced autograders with sample student submissions
- **Action:** Train TAs on new autograding system and how to support students
- **Deliverable:** Production-ready enhanced autograders for target courses

### August 2026 (Pre-Semester)
- **Milestone:** Launch readiness achieved
- **Action:** Final faculty prep meeting and Q&A
- **Action:** Distribute student guides on Claude usage and academic integrity
- **Action:** Set up course-specific Claude usage guidelines in Learning Management System
- **Action:** Prepare orientation materials for first day of class
- **Action:** Establish weekly faculty check-in schedule for ongoing support
- **Deliverable:** All courses ready for Day 1 of Fall semester

## Phase 3: Fall 2026 Launch & Support (August - December 2026)

**Objective:** Execute successful launch with continuous support and monitoring

### August - September 2026
- **Milestone:** Successful course launch and student onboarding
- **Action:** First-week orientation on AI tools and academic integrity in all three courses
- **Action:** Students receive access to Claude with clear usage guidelines
- **Action:** First assignments use "encouraged" tier to build comfort with AI collaboration
- **Action:** Collect early feedback from students on Claude access and clarity of policies
- **Action:** Weekly faculty working group meetings to share experiences and troubleshoot
- **Deliverable:** Students successfully onboarded to AI-assisted coursework

### October - November 2026
- **Milestone:** Mid-semester assessment and adjustment
- **Action:** Review autograder effectiveness (are they assessing understanding?)
- **Action:** Monitor academic integrity (any concerns or patterns?)
- **Action:** Gather student feedback through mid-semester surveys
- **Action:** Adjust assignment tiers or policies based on early experience
- **Action:** Document successful practices and challenges for future semesters
- **Deliverable:** Mid-semester progress report with adjustments made

### December 2026
- **Milestone:** End-of-semester data collection and analysis
- **Action:** Collect final student surveys on Claude usage and learning impact
- **Action:** Faculty debrief on what worked, what didn't, and what to change
- **Action:** Analyze academic performance data (do students using Claude learn effectively?)
- **Action:** Review academic integrity incidents related to AI use
- **Action:** Compile case studies and examples for sharing with broader faculty
- **Deliverable:** Fall 2026 Final Report with recommendations for expansion

## Phase 4: Assessment & Planning for Expansion (December 2026)

**Objective:** Evaluate pilot success and plan broader rollout

### December 2026 - January 2027
- **Milestone:** Pilot evaluation complete
- **Action:** Analyze student learning outcomes (performance on AI-assisted vs. independent assessments)
- **Action:** Evaluate faculty satisfaction and readiness to continue
- **Action:** Assess autograding effectiveness at scale
- **Action:** Calculate ROI (student outcomes improvement vs. license and development costs)
- **Action:** Present findings to department for decisions on expansion
- **Deliverable:** Comprehensive pilot evaluation report

### Planning for Spring/Fall 2027 Expansion
- **Action:** Identify additional courses for next phase based on pilot success
- **Action:** Refine policies and assignment templates based on lessons learned
- **Action:** Develop faculty training materials for wider adoption
- **Action:** Consider publishing or presenting findings to broader CS education community
- **Deliverable:** Expansion plan for 2027-2028 academic year

## Critical Milestones Summary

| Date | Milestone | Responsible Party |
|------|-----------|-------------------|
| Jan 2026 | Department approval and faculty leads identified | Department Chair + Faculty Leads |
| Feb 2026 | Claude licenses secured and accessible | IT Coordinator + Budget Office |
| Mar 2026 | AI usage policy approved | Faculty Working Group + Honor Code Office |
| Apr 2026 | Autograder enhancement plan finalized | Technical Lead + Course Instructors |
| May 2026 | Faculty training workshops completed | Instructional Design + Faculty Leads |
| Jun 2026 | Course materials redesigned | Course Instructors |
| Jul 2026 | Enhanced autograders tested and ready | Technical Lead + TAs |
| Aug 2026 | Fall semester launch | All faculty and TAs |
| Oct 2026 | Mid-semester assessment complete | Faculty Working Group |
| Dec 2026 | Pilot evaluation and expansion planning | Department Leadership |

## Key Roles and Responsibilities

**Department Chair / Leadership**
- Approve plan and budget
- Communicate vision to faculty and students
- Make decisions on expansion based on pilot results

**Faculty Leads (CS 260, CS 340, CS 329 Instructors)**
- Redesign courses with AI integration
- Implement AI usage policies in their courses
- Provide regular feedback on effectiveness
- Participate in working group

**AI Integration Coordinator** (Recommended: Designate one faculty member)
- Lead faculty workshops
- Coordinate autograder development
- Support faculty during implementation
- Compile assessment data and reports

**Technical Lead / Autograding Specialist**
- Enhance autograding systems
- Integrate LLM-based assessment
- Train TAs on new systems
- Troubleshoot technical issues

**Instructional Design Support**
- Assist faculty with assignment redesign
- Develop student-facing resources
- Create assessment rubrics
- Document best practices

**TAs (Teaching Assistants)**
- Learn enhanced autograding system
- Support students with Claude usage
- Monitor for academic integrity concerns
- Provide feedback on student experience

## Success Metrics

### Student Outcomes
- **Learning effectiveness:** Students demonstrate understanding on both AI-assisted and independent assessments
- **Skill development:** Students can explain design decisions and verify AI-generated code
- **Professional readiness:** Exit surveys show confidence in using AI tools professionally
- **Satisfaction:** 80%+ of students report positive experience with Claude integration

### Faculty Outcomes
- **Confidence:** Faculty feel prepared to teach with AI tools
- **Course quality:** Faculty report ability to cover more advanced material or assign more ambitious projects
- **Sustainability:** Faculty willing to continue AI integration in future semesters
- **Workload:** Faculty workload is manageable with enhanced autograding

### Department Outcomes
- **Academic integrity:** No significant increase in academic integrity violations
- **Scalability:** Autograding successfully handles 1,500 students with 40 faculty
- **Leadership:** BYU CS recognized for innovative AI pedagogy
- **Replicability:** Other departments express interest in adopting approach

### Measurable Targets
- 200-300 students successfully complete courses with Claude integration
- Average student satisfaction rating ≥ 4.0/5.0 on AI integration
- Faculty report ≤10% increase in grading workload despite more sophisticated assessment
- Academic integrity incident rate remains flat or decreases
- 100% of pilot faculty willing to continue in Spring 2027

## Risk Mitigation

### Risk: Students over-rely on AI and don't develop fundamental skills
**Mitigation:**
- Multi-dimensional autograding tests understanding, not just correctness
- Periodic "no AI" assessments to verify independent capability
- Progressive scaffolding (limit AI early, increase later in semester)

### Risk: Autograder development takes longer than expected
**Mitigation:**
- Start autograder work in April with 3-month buffer before launch
- Prioritize most critical enhancements; defer nice-to-haves if needed
- Have manual grading backup plan for subset of assignments

### Risk: Faculty resistance or insufficient training
**Mitigation:**
- Target courses with enthusiastic faculty for pilot (CS 260 instructor eager)
- Provide stipends and course release time for redesign work
- Ongoing support through weekly working group meetings
- Clear documentation and templates to reduce burden

### Risk: Budget constraints or license access issues
**Mitigation:**
- Budget already allocated (confirmed)
- Negotiate educational pricing with Anthropic
- Have free-tier fallback option for students if needed
- Computer lab access for students without personal subscriptions

### Risk: Academic integrity incidents spike
**Mitigation:**
- Clear policies from Day 1 integrated with BYU Honor Code
- Autograders designed to be cheat-resistant (multi-dimensional assessment)
- Regular faculty check-ins to identify and address concerns early
- Student education on appropriate vs. inappropriate AI use

### Risk: University-level policy changes restrict AI use
**Mitigation:**
- Engage with university administration early (March 2026)
- Position as professional skills development, not just convenience
- Document pedagogical rationale and alignment with learning outcomes
- Build relationships with other departments doing AI integration

## Budget Considerations

**Estimated Costs for Fall 2026 Pilot:**

**Claude Licenses**
- 250 students × $30/month (Claude Team) × 4 months = $30,000
- Alternative: Negotiate annual educational license for lower per-student cost

**Faculty Development**
- Summer stipends for 3 faculty course redesign (3 × $3,000) = $9,000
- Workshop facilitation and materials = $2,000

**Autograding Development**
- Technical development (contractor or TA hours) = $10,000-15,000
- Claude API costs for explanation assessment = $1,000-2,000

**Support and Administration**
- AI Integration Coordinator (course release or stipend) = $5,000
- Instructional design support = $3,000

**Total Estimated Budget: $60,000-66,000 for Fall 2026 pilot**

**Notes:**
- Budget already allocated per initial discussions
- Costs decrease significantly in subsequent semesters (autograders built, faculty trained)
- Potential for external funding or educational grants given innovative nature

## Next Steps (Immediate Actions)

1. **Schedule department meeting** to present recommendation and implementation plan (Target: Late January 2026)

2. **Identify specific faculty members** for CS 260, CS 340, CS 329 who will lead pilot

3. **Initiate license procurement process** with Anthropic and BYU procurement office

4. **Form AI Integration Working Group** with representatives from:
   - Faculty leads from target courses
   - Department leadership
   - Instructional design
   - TA coordinator
   - Technical/autograding specialist

5. **Assign AI Integration Coordinator role** to manage implementation

6. **Schedule initial working group meeting** to kick off planning phase (Target: February 2026)

## Appendices

### Appendix A: Sample AI Usage Policy Template
(See Recommendation document Section 2.1 for detailed usage framework)

### Appendix B: Assignment Redesign Checklist
- [ ] Define which tier (encouraged/allowed/prohibited) applies to this assignment
- [ ] Specify learning objectives being assessed
- [ ] Identify what students must demonstrate understanding of independently
- [ ] Design autograder to test multiple dimensions (not just correctness)
- [ ] Create rubric for explanation/documentation requirements
- [ ] Add constraint requirements if applicable (recursion-only, pattern usage, etc.)
- [ ] Write student-facing instructions with clear AI usage guidelines
- [ ] Prepare sample solutions showing good vs. poor AI collaboration

### Appendix C: Student Guide Outline: "Working Effectively with Claude"
1. What Claude can help with (and what it can't)
2. When to use Claude (and when to work independently)
3. How to prompt effectively
4. Verifying and testing AI-generated code
5. Academic integrity and honor code expectations
6. Getting help when Claude isn't sufficient

### Appendix D: Faculty Support Resources
- Weekly working group meetings (Fridays during Fall 2026)
- Shared assignment repository
- Slack channel for quick questions
- Office hours with AI Integration Coordinator
- Documentation wiki for troubleshooting

### Appendix E: Using Claude API for Autograding Design Documents

One of the most powerful applications of AI in education is using Claude itself to grade subjective aspects of student work at scale. This addresses the 1,500:40 student-to-faculty ratio while assessing higher-order thinking.

#### How It Works

**Architecture:**
```
Student submits → Autograder system → Calls Claude API → Returns structured assessment → Recorded in gradebook
```

**Example: Grading a Software Design Document**

```python
import anthropic

def grade_design_document(student_code, design_doc, rubric):
    """
    Use Claude API to assess whether design document demonstrates understanding.
    """
    client = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))

    prompt = f"""You are an expert software engineering instructor grading a student's design document.

ASSIGNMENT: Design and implement a task management system

STUDENT'S DESIGN DOCUMENT:
{design_doc}

STUDENT'S IMPLEMENTATION CODE:
{student_code}

GRADING RUBRIC (40 points total):
1. Design Justification (10 pts): Does student explain WHY they chose this architecture/pattern?
2. Tradeoff Analysis (10 pts): Does student discuss alternatives and explain tradeoffs?
3. Design-Code Alignment (10 pts): Does implementation match the documented design?
4. Depth of Understanding (10 pts): Does explanation go beyond surface-level description?

For each criterion:
- Provide a score (0-10)
- Explain what's strong and what's missing
- Identify if explanation seems AI-generated without genuine understanding
- Note specific examples from the document

Return your assessment in this JSON format:
{{
    "design_justification": {{
        "score": <0-10>,
        "feedback": "<specific feedback>",
        "evidence": "<quote from document>"
    }},
    "tradeoff_analysis": {{
        "score": <0-10>,
        "feedback": "<specific feedback>",
        "evidence": "<quote from document>"
    }},
    "design_code_alignment": {{
        "score": <0-10>,
        "feedback": "<specific feedback>",
        "mismatches": ["<any inconsistencies>"]
    }},
    "depth_of_understanding": {{
        "score": <0-10>,
        "feedback": "<specific feedback>",
        "concerns": "<if explanation seems copied/AI-generated without understanding>"
    }},
    "total_score": <sum of all scores>,
    "overall_feedback": "<summary for student>",
    "red_flags": ["<any academic integrity concerns>"]
}}
"""

    message = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=2000,
        messages=[{"role": "user", "content": prompt}]
    )

    # Parse JSON response
    assessment = json.loads(message.content[0].text)
    return assessment
```

#### What Claude Can Assess

**Design Quality Indicators:**
1. **Justification depth:** Does student explain reasoning, or just describe what they did?
2. **Tradeoff awareness:** Does student discuss alternatives and why they chose one approach?
3. **Technical accuracy:** Are design patterns correctly identified and applied?
4. **Alignment with code:** Does implementation match documented design?
5. **Appropriate complexity:** Is solution neither over-engineered nor too simplistic?

**Red Flags for Academic Dishonesty:**
- Generic explanations that could apply to any project
- Missing specific details from their own implementation
- Sophisticated terminology without demonstrating understanding
- Design document doesn't match actual code structure
- Explanation contradicts what code actually does

**Example Assessments:**

**High-quality explanation (9/10):**
> "I chose the Observer pattern because the UI needs to update whenever task status changes. I considered using polling but rejected it due to inefficiency - with 1000+ tasks, constant polling would waste CPU cycles. Observer is more complex to implement but provides real-time updates with minimal overhead. The tradeoff is increased coupling between TaskManager and UI components, which I mitigated by using an event bus interface."

Claude assessment: *Student demonstrates clear understanding of pattern choice, discusses rejected alternatives with technical reasoning, identifies specific tradeoffs, and shows awareness of implementation challenges.*

**Low-quality explanation (3/10):**
> "I used the Observer pattern because it's good for this type of application. It allows objects to communicate efficiently and makes the code more maintainable. This is a widely-used pattern in software engineering."

Claude assessment: *Generic explanation with no specific justification for this project. Doesn't discuss alternatives or tradeoffs. Could be copied from documentation. Student needs to explain WHY Observer fits their specific requirements.*

#### Implementation Strategies

**Calibration Phase:**
1. Have faculty manually grade 20-30 design documents
2. Run same documents through Claude API grading
3. Compare scores and identify discrepancies
4. Refine prompts to align with faculty expectations
5. Establish confidence threshold (e.g., if Claude is < 70% confident, flag for manual review)

**Production Use:**
1. All design documents auto-graded by Claude
2. Scores ≥ 70%: Automated grade recorded, feedback sent to student
3. Scores 50-70%: Automated grade recorded, flagged for spot-check review
4. Scores < 50%: Held for full manual review
5. Random 10% sample manually reviewed for quality control

**Cost Considerations:**
- Claude API cost: ~$0.10-0.30 per design document assessment (depending on document length)
- For 300 students × 5 design assignments = 1,500 assessments = ~$150-450 per semester
- Compared to 40 hours of TA time at $15/hour = $600 saved
- Plus: Faster feedback turnaround for students

#### Benefits

1. **Scalable subjective assessment:** Can grade design quality for 1,500 students
2. **Consistent rubric application:** Claude applies rubric uniformly
3. **Detailed feedback:** Students get specific, actionable feedback on every submission
4. **Academic integrity detection:** Claude can flag suspicious explanations
5. **Faculty time savings:** Instructors review flagged cases only, not all 1,500 submissions

### Appendix F: Cheat-Resistant Assignment Patterns

These assignment types require genuine understanding and cannot be "solved" by simply using Claude to generate code. Students can use Claude as a tool, but must demonstrate learning across multiple dimensions.

#### Pattern 1: Comparative Implementation with Analysis

**Assignment Structure:**
- Implement the same functionality using TWO different approaches
- Write analysis comparing approaches with specific metrics
- Autograder validates both implementations and assesses analysis quality

**Example (CS 340 - Software Design):**
```
Assignment: Task Queue System

Part 1: Implement task queue using:
  - Version A: Array-based circular buffer
  - Version B: Linked list

Part 2: Benchmark both implementations:
  - Measure performance with 1000, 10000, 100000 tasks
  - Compare memory usage
  - Document results in tables/graphs

Part 3: Analysis (submitted as design document, graded by Claude API):
  - When would you choose Version A vs. B in production?
  - Explain time complexity of each operation (enqueue, dequeue, peek)
  - Discuss cache locality and why it matters
  - Recommend which to use for specific scenarios (high throughput, memory-constrained, etc.)

Autograder checks:
  ✓ Both versions implement interface correctly
  ✓ Performance benchmarks actually measure both versions
  ✓ Analysis references student's specific measurements (not generic)
  ✓ Complexity analysis is technically accurate
  ✓ Recommendations are justified by data
```

**Why it's cheat-resistant:**
- Claude can help implement both versions, but student must run benchmarks
- Analysis must reference student's actual performance data
- Generic AI-generated analysis won't match their specific results
- Demonstrates understanding through comparison and tradeoff reasoning

---

#### Pattern 2: Code Archaeology and Refactoring

**Assignment Structure:**
- Provide students with intentionally flawed or legacy code
- Students must identify issues, explain problems, and improve code
- Tests understanding of code review and design principles

**Example (CS 340 - Software Design):**
```
Assignment: Legacy E-commerce System Refactoring

Given: 500-line "legacy" codebase with multiple issues:
  - God object (ShopManager does everything)
  - No error handling
  - Tight coupling (UI directly calls database)
  - No tests
  - Security vulnerabilities (SQL injection)
  - Poor naming and magic numbers

Part 1: Code Review (design document graded by Claude API):
  - Identify at least 5 design problems
  - Explain WHY each is problematic (not just "bad practice")
  - Prioritize issues by severity and impact
  - Propose specific refactoring strategy

Part 2: Implementation:
  - Refactor to improve design (separation of concerns, dependency injection, etc.)
  - Add comprehensive tests
  - Fix security issues
  - Preserve all original functionality

Autograder checks:
  ✓ All original features still work (functional correctness)
  ✓ Code now follows SOLID principles (architectural checks)
  ✓ Tests cover critical paths (test quality assessment)
  ✓ Security vulnerabilities fixed (static analysis)
  ✓ Code review document demonstrates understanding of problems
```

**Why it's cheat-resistant:**
- Claude can help refactor, but student must identify and explain problems first
- Requires reading and understanding existing code, not just generating new code
- Explanation must reference specific lines from the provided codebase
- Tests critical thinking about design quality, not just implementation

---

#### Pattern 3: Constrained Implementation

**Assignment Structure:**
- Implement solution with specific constraints that test understanding
- Constraints chosen to prevent simple AI delegation

**Example (CS 260 - Web Programming):**
```
Assignment: Build a Single Page App WITHOUT Using Frameworks

Constraints:
  - No React, Vue, Angular, or other frameworks
  - Must use vanilla JavaScript only
  - Implement your own component system with state management
  - Implement your own router for multiple views
  - Must handle browser back/forward buttons correctly

Required Features:
  - Multiple views (list, detail, create)
  - Client-side routing
  - State persistence (localStorage)
  - Form validation

Part 1: Architecture Document (Claude-graded):
  - Explain your component architecture
  - How does state management work?
  - How does routing work?
  - Why are these patterns necessary? (demonstrates understanding of what frameworks do)

Part 2: Implementation

Autograder checks:
  ✓ No framework libraries imported (dependency check)
  ✓ Custom component system implemented
  ✓ Routing works without page refresh
  ✓ Back button functionality works
  ✓ Explanation demonstrates understanding of framework internals
```

**Why it's cheat-resistant:**
- Claude can help implement, but constraints force deep engagement with fundamentals
- Student must understand what frameworks do "under the hood"
- Explanation required to articulate why patterns are necessary
- Cannot just accept AI-generated framework boilerplate

---

#### Pattern 4: Test-First Development with Quality Requirements

**Assignment Structure:**
- Require comprehensive tests BEFORE implementation
- Grade test quality, not just code correctness
- Tests reveal understanding of requirements and edge cases

**Example (CS 329 - QA and DevOps):**
```
Assignment: API Testing and Quality Assurance

Part 1: Write comprehensive test suite for [provided API spec]
  - Unit tests for business logic
  - Integration tests for API endpoints
  - Edge cases and error conditions
  - Performance tests
  - Security tests (authentication, authorization, input validation)

Part 2: Justify test strategy (Claude-graded document):
  - Why did you choose these test cases?
  - What edge cases are you testing and why?
  - How did you achieve X% coverage?
  - What scenarios are NOT tested and why?

Part 3: Implement API to pass your tests

Autograder checks:
  ✓ Test coverage ≥ 85%
  ✓ Tests include edge cases (empty inputs, null values, boundary conditions)
  ✓ Tests actually fail when code is wrong (mutation testing)
  ✓ Performance tests validate response time requirements
  ✓ Security tests check for common vulnerabilities
  ✓ Explanation demonstrates testing strategy, not just description
```

**Why it's cheat-resistant:**
- Good tests require understanding requirements deeply
- Claude can help write tests, but student must decide WHAT to test
- Test quality reveals understanding of edge cases and failure modes
- Justification document prevents blind acceptance of AI-generated tests

---

#### Pattern 5: Design-First with Peer Review

**Assignment Structure:**
- Multi-stage assignment with design approval before implementation
- Peer review component requires critical evaluation skills

**Example (CS 340 - Software Design):**
```
Assignment: Multi-Stage Project with Design Gates

Stage 1: Design Proposal (Week 1)
  - Submit architecture diagram and design document
  - Explain pattern choices and justify architecture
  - Faculty/Claude provide feedback
  - Must be approved before proceeding

Stage 2: Implementation (Weeks 2-3)
  - Implement according to approved design
  - Claude assistance allowed for implementation
  - Must match approved architecture

Stage 3: Peer Code Review (Week 4)
  - Review 2 other students' implementations
  - Evaluate: Does code match their design? Is design well-executed?
  - Provide constructive feedback

Stage 4: Revision (Week 5)
  - Address peer feedback
  - Justify which feedback you incorporated and why

Autograder checks:
  ✓ Implementation matches approved design
  ✓ Peer reviews demonstrate critical evaluation (Claude-graded)
  ✓ Revision justification shows reasoning about feedback
```

**Why it's cheat-resistant:**
- Design must be defended before implementation
- Cannot just generate and submit code
- Peer review requires critical thinking about others' work
- Revision justification reveals understanding of design decisions

---

#### Pattern 6: Debugging and Root Cause Analysis

**Assignment Structure:**
- Provide buggy code and failing tests
- Students must diagnose, explain, and fix

**Example (CS 260 - Web Programming):**
```
Assignment: Debug a Broken Web Application

Given: Web app with 5 subtle bugs:
  1. Race condition in async code
  2. Memory leak in event listeners
  3. Incorrect state update causing UI bug
  4. CORS misconfiguration
  5. Off-by-one error in pagination

Part 1: Root Cause Analysis (Claude-graded):
  - For each bug, explain:
    * What is the symptom?
    * What is the root cause?
    * Why does this happen? (technical explanation)
    * How did you diagnose it?

Part 2: Fix Implementation:
  - Fix all bugs
  - Add tests to prevent regression
  - Document prevention strategies

Autograder checks:
  ✓ All bugs fixed (functional correctness)
  ✓ Regression tests added
  ✓ Root cause analysis demonstrates debugging understanding
  ✓ Explanations are technically accurate
```

**Why it's cheat-resistant:**
- Requires reading and understanding existing code
- Claude can suggest fixes, but student must explain diagnosis process
- Root cause analysis reveals debugging thought process
- Cannot just generate new code - must understand what's broken

---

#### Pattern 7: Performance Optimization with Profiling

**Assignment Structure:**
- Provide inefficient but correct code
- Require optimization with measurable improvement
- Must justify optimizations with profiling data

**Example (CS 340 - Software Design):**
```
Assignment: Optimize a Slow System

Given: Correct but inefficient implementation (O(n²) where O(n log n) possible)

Part 1: Profiling and Analysis:
  - Profile code to identify bottlenecks
  - Measure baseline performance
  - Identify algorithmic complexity issues
  - Propose optimization strategy

Part 2: Optimization Implementation:
  - Implement optimizations
  - Measure performance improvement
  - Achieve ≥ 10x speedup on large inputs

Part 3: Justification (Claude-graded):
  - Explain what was slow and why
  - Explain optimization technique and why it works
  - Show profiling data before/after
  - Discuss space/time tradeoffs

Autograder checks:
  ✓ Correctness maintained
  ✓ Performance improvement verified (benchmarks)
  ✓ Profiling data included
  ✓ Explanation demonstrates algorithmic understanding
```

**Why it's cheat-resistant:**
- Must run profiling tools and collect actual data
- Explanation must reference student's specific profiling results
- Performance improvement must be measured, not claimed
- Demonstrates understanding of algorithmic complexity

---

#### Common Patterns Across Cheat-Resistant Assignments

1. **Multi-dimensional requirements:** Code + explanation + data + analysis
2. **Require student-specific artifacts:** Profiling data, benchmark results, specific code references
3. **Justification of decisions:** Not just what, but why and what alternatives were considered
4. **Process documentation:** Show your work, not just final answer
5. **Critical evaluation:** Review, compare, or debug rather than just generate
6. **Constraint-based:** Require specific approaches that test understanding

**Key Principle:** Design assignments where Claude is a useful tool but cannot replace the thinking, analysis, and decision-making that demonstrates learning.

### Appendix G: Building Faculty Confidence with Claude

**Challenge:** Faculty may feel uncertain about using Claude themselves, let alone teaching with it. Many may worry they lack the expertise to guide students in AI-assisted development.

**Solution:** Structured hands-on learning experiences that build confidence through practical use.

---

## Why Faculty Confidence Matters

Faculty who are confident Claude users will:
- Design better AI-appropriate assignments
- Guide students more effectively when they get stuck
- Recognize when students are over-relying on AI vs. using it appropriately
- Model professional AI collaboration for students
- Troubleshoot issues during the semester
- Feel empowered rather than threatened by AI tools

**Key Insight:** Faculty don't need to be "Claude experts" - they need to be confident users who understand Claude's strengths, limitations, and appropriate use cases.

---

## Faculty Learning Path: From Novice to Confident User

### Phase 1: Personal Exploration (Week 1-2)
**Goal:** Get comfortable with basic Claude interactions

**Activities:**
1. **Sign up and explore** - Create Claude.ai account, try free tier
2. **Daily practice** - Use Claude for 15 minutes/day on personal tasks:
   - "Explain this research paper abstract in simple terms"
   - "Help me debug this Python script I wrote for data analysis"
   - "Review my draft email to the department for clarity"
   - "Suggest 5 creative examples to explain recursion to beginners"

3. **Keep a learning journal** - Document:
   - What worked well
   - Where Claude struggled
   - Surprising capabilities or limitations
   - Ideas for classroom use

**Outcome:** Faculty experience Claude as a helpful tool, not a mysterious black box.

---

### Phase 2: Teaching-Related Tasks (Week 3-4)
**Goal:** Use Claude for actual faculty work to see benefits firsthand

**Suggested Faculty Projects:**

#### Project 1: Assignment Redesign with Claude
**Task:** Take an existing assignment and discuss with Claude how to make it AI-appropriate.

**Process:**
```
Faculty → Claude: "I have an assignment where students implement a binary search
tree in Java. How can I redesign this for an environment where students can use
AI assistants, while still ensuring they learn data structures?"

Claude → Faculty: [Suggests comparative implementation, explanation requirements,
constraint-based approaches, etc.]

Faculty → Claude: "The constraint-based idea is interesting. Can you help me
design specific constraints that would test understanding?"

[Continues discussion, iterating on assignment design]
```

**What faculty learn:**
- How to have design conversations with Claude
- Claude can generate ideas but faculty makes final decisions
- Iterative prompting yields better results
- Claude understands pedagogical goals

**Deliverable:** Redesigned assignment ready to pilot

---

#### Project 2: Generate Test Cases with Claude
**Task:** Use Claude to generate comprehensive test cases for an assignment autograder.

**Process:**
```
Faculty → Claude: "I need to create test cases for a student assignment where
they implement a REST API for a library system. The API should handle books,
users, and checkouts. What test cases should I include?"

Claude → Faculty: [Lists functional tests, edge cases, error conditions,
security tests]

Faculty → Claude: "Good start. I especially need tests for the checkout logic -
can you generate specific test cases for edge cases like: checking out a book
that's already checked out, returning a book that wasn't checked out, etc.?"

Claude → Faculty: [Generates specific test code]

Faculty: [Reviews, modifies, and adds to autograder]
```

**What faculty learn:**
- Claude can accelerate tedious tasks (test case generation)
- Still need to review and verify Claude's output
- Specific prompts yield better results than generic ones
- Claude understands technical requirements

**Deliverable:** Comprehensive test suite for autograder

---

#### Project 3: Create Rubric with Claude
**Task:** Design a grading rubric for a design document or project reflection.

**Process:**
```
Faculty → Claude: "I need a rubric to grade student design documents for a
web application project. Students should explain their architecture, database
schema, and API design. The rubric should assess whether they understand their
choices, not just whether they made correct choices. Help me design this."

Claude → Faculty: [Suggests rubric dimensions, point allocations, descriptors
for each level]

Faculty → Claude: "I like this, but I want to emphasize justification and
tradeoff analysis more. Can you revise?"

[Iterate until satisfied]
```

**What faculty learn:**
- Claude can help structure rubrics
- Faculty expertise guides the criteria
- Rubrics can assess understanding vs. correctness
- Claude understands educational assessment

**Deliverable:** Clear rubric for subjective assessment

---

#### Project 4: Build Sample Solutions
**Task:** Use Claude to help create reference implementations for assignments.

**Process:**
```
Faculty → Claude: "I'm creating a sample solution for a student assignment:
implement a task queue using both an array-based circular buffer and a linked
list. Can you help me implement both versions?"

Claude → Faculty: [Generates both implementations]

Faculty: [Reviews code, tests it, identifies issues]

Faculty → Claude: "The linked list version doesn't handle the case where the
queue is empty. Can you add error handling?"

Claude → Faculty: [Adds error handling]

Faculty: [Continues refining until it matches expected quality]
```

**What faculty learn:**
- Claude can generate working code quickly
- Faculty must verify correctness (doesn't replace testing)
- Can iterate to improve code quality
- Helps create multiple solution approaches for comparison

**Deliverable:** Reference implementations for assignment

---

#### Project 5: Design Lecture Examples
**Task:** Generate code examples or explanations for lecture topics.

**Process:**
```
Faculty → Claude: "I'm teaching about design patterns next week. I need a
concrete example of the Observer pattern that students can relate to. Preferably
something more interesting than the typical weather station example."

Claude → Faculty: [Suggests social media notification system, stock price
tracker, collaborative document editor, etc.]

Faculty → Claude: "I like the social media notification idea. Can you implement
a simple version in Python with detailed comments explaining how the Observer
pattern works?"

Claude → Faculty: [Generates commented code example]
```

**What faculty learn:**
- Claude can generate creative examples
- Can tailor examples to specific learning objectives
- Code explanations can supplement lectures
- Useful for generating multiple examples quickly

**Deliverable:** Engaging lecture examples

---

### Phase 3: Collaborative Learning (Week 5-6)
**Goal:** Faculty learn from each other's experiences

**Activities:**

#### Faculty Sharing Sessions
- **Weekly roundtable:** Faculty share:
  - "This week I used Claude to..."
  - "Something that worked well..."
  - "Something that didn't work..."
  - "A limitation I discovered..."

#### Peer Observation
- Faculty observe each other using Claude for tasks
- Share prompting strategies
- Discuss different approaches to the same problem

#### Shared Prompt Library
- Create department repository of effective prompts:
  - Assignment redesign prompts
  - Test case generation prompts
  - Code review prompts
  - Explanation generation prompts

**Outcome:** Community of practice around AI-assisted teaching

---

## Best Practices for Learning Claude

### 1. Start with Your Own Work
**Don't:** Start by designing student assignments
**Do:** Use Claude for your own tasks first (grading, research, coding)
**Why:** Lower stakes, immediate feedback on what works

### 2. Iterate Your Prompts
**Don't:** Expect perfect results from first prompt
**Do:** Treat conversation as iterative refinement
**Why:** Claude gets better with context and clarification

**Example progression:**
```
Prompt 1: "Help me design a database"
→ Too vague, Claude asks clarifying questions

Prompt 2: "Help me design a database schema for a restaurant review app with
restaurants and reviews"
→ Better, Claude provides basic schema

Prompt 3: "The schema looks good. Should reviews be embedded in restaurant
documents or in a separate collection? I'm using MongoDB and expect restaurants
to have hundreds of reviews."
→ Claude discusses tradeoffs specific to your context
```

### 3. Verify, Don't Trust Blindly
**Don't:** Copy-paste Claude's code without understanding
**Do:** Test, verify, and understand before using
**Why:** Claude makes mistakes; you're responsible for correctness

**Practice:**
- Read all generated code
- Test with edge cases
- Ask Claude to explain parts you don't understand
- Modify to fit your exact needs

### 4. Use Claude for Brainstorming, Not Final Decisions
**Don't:** Accept Claude's first suggestion as the answer
**Do:** Use Claude to generate options, then apply your expertise
**Why:** You understand context Claude doesn't (student level, course goals, time constraints)

### 5. Be Specific
**Don't:** "Make this code better"
**Do:** "Refactor this code to use the Strategy pattern, add comments explaining the pattern, and handle the case where no strategy is provided"
**Why:** Specific prompts yield useful results

### 6. Ask Claude to Explain Its Reasoning
**Helpful prompts:**
- "Why did you choose this approach?"
- "What are the tradeoffs of this solution?"
- "What alternative approaches exist?"
- "Explain this code line by line"

### 7. Use Claude as a Thinking Partner
**Effective patterns:**
- "I'm considering two approaches: A and B. Help me analyze the tradeoffs"
- "Here's my current design. What potential issues do you see?"
- "I'm stuck on X. Can you suggest approaches I might not have considered?"

---

## Recommended Training Resources

### 1. Anthropic Official Resources

**Claude.ai Quick Start Guide**
- Interactive tutorial when you first sign up
- Covers basic prompting, conversation features, file uploads
- ~15 minutes
- **Link:** Available at claude.ai after sign-up

**Anthropic Documentation**
- Comprehensive guide to Claude's capabilities
- Best practices for prompting
- API documentation (for autograding use)
- **Link:** docs.anthropic.com

**Prompt Engineering Guide**
- Techniques for effective prompting
- Examples for different tasks
- Common pitfalls to avoid
- **Recommended sections for faculty:**
  - "Be clear and direct"
  - "Use examples (multishot prompting)"
  - "Give Claude a role"
  - "Use XML tags for structure"

### 2. Self-Paced Learning Activities

**Week 1-2: Foundations**
- Sign up for Claude.ai (free tier sufficient for learning)
- Complete built-in tutorial
- Read: "Prompt Engineering Guide" (30 min)
- Practice: Use Claude for 3 personal tasks daily

**Week 3-4: Teaching Applications**
- Complete one faculty project (from list above)
- Read: "Claude for Education" use cases
- Practice: Redesign one existing assignment with Claude's help

**Week 5-6: Advanced Techniques**
- Experiment with longer conversations (building context)
- Try file uploads (give Claude your syllabus to analyze)
- Practice: Create test cases or rubrics with Claude
- Attend: Faculty sharing session to learn from peers

### 3. BYU-Specific Training Workshop Series

**Proposed Workshop Series (May 2026)**

#### Workshop 1: "Getting Started with Claude" (2 hours)
**Format:** Hands-on computer lab session

**Agenda:**
- 0:00-0:15: Introduction and account setup
- 0:15-0:45: Guided exercises:
  - Basic conversation
  - Iterative prompting
  - Code generation and explanation
  - File uploads
- 0:45-1:15: Faculty projects (hands-on):
  - Each faculty picks one task (assignment redesign, test generation, etc.)
  - Work individually with Claude
  - Facilitator circulates to help
- 1:15-1:45: Share and discuss:
  - What worked well?
  - What was surprising?
  - What didn't work as expected?
- 1:45-2:00: Best practices recap and Q&A

**Deliverable:** Each faculty completes one meaningful task using Claude

---

#### Workshop 2: "Designing AI-Era Assignments" (2 hours)
**Format:** Design studio

**Agenda:**
- 0:00-0:20: Presentation on assignment design principles
  - Multi-dimensional assessment
  - Cheat-resistant patterns
  - Examples from CS 260, 340, 329
- 0:20-1:00: Hands-on redesign:
  - Faculty bring existing assignment
  - Use Claude to brainstorm redesign ideas
  - Apply cheat-resistant patterns
  - Design rubric with Claude
- 1:00-1:30: Peer review:
  - Small groups (3-4 faculty)
  - Share redesigned assignments
  - Give feedback
  - Discuss challenges
- 1:30-2:00: Whole group sharing and refinement

**Deliverable:** Draft of AI-appropriate assignment for each faculty member

---

#### Workshop 3: "Autograding with Claude API" (2 hours)
**Format:** Technical training

**Agenda:**
- 0:00-0:30: Introduction to Claude API
  - API basics and authentication
  - Differences from chat interface
  - Cost considerations
  - Demo: Grading a design document via API
- 0:30-1:00: Hands-on API practice:
  - Set up API key
  - Write simple grading script
  - Test with sample student work
- 1:00-1:30: Rubric design for automated grading:
  - What can Claude assess well?
  - What needs human review?
  - Calibration process
  - Quality control strategies
- 1:30-2:00: Planning session:
  - Which assignments will use Claude API grading?
  - Timeline for implementation
  - Support needed

**Deliverable:** Working autograding script for at least one assignment component

---

#### Workshop 4: "Academic Integrity in the AI Era" (1.5 hours)
**Format:** Discussion and policy workshop

**Agenda:**
- 0:00-0:20: Framing the issue
  - Traditional academic integrity vs. AI era
  - BYU Honor Code considerations
  - Student perspectives on AI use
- 0:20-0:50: Hands-on: Distinguishing good vs. problematic AI use
  - Review sample student submissions
  - Practice identifying over-reliance on AI
  - Discuss borderline cases
- 0:50-1:20: Syllabus language workshop:
  - Draft AI usage policies for syllabi
  - Create assignment tiers (encouraged/allowed/prohibited)
  - Develop student guidelines
- 1:20-1:30: Commitment and next steps

**Deliverable:** Draft syllabus language on AI usage

---

### 4. Ongoing Support Structures

**Weekly Office Hours (Fall 2026)**
- AI Integration Coordinator holds weekly office hours
- Faculty drop in with questions, challenges, or to share successes
- Troubleshoot issues together

**Slack/Teams Channel**
- Async communication for quick questions
- Share prompts that worked well
- Discuss student situations
- Celebrate successes

**Monthly Faculty Learning Community**
- 1-hour lunch meeting
- Share what's working in classes
- Discuss challenges
- Guest speakers (industry professionals on AI in development)

---

## Building Confidence Through Success Stories

**Strategy:** Help faculty see concrete benefits early

### Quick Wins (First 2 Weeks)

**Win 1: "Claude helped me redesign an assignment in 30 minutes"**
- Old assignment: "Implement binary search tree"
- New assignment: "Implement BST and AVL tree, benchmark both, analyze tradeoffs"
- Claude helped: Generate the AVL tree reference code, suggest benchmark criteria, draft rubric

**Win 2: "Claude generated 20 test cases I hadn't thought of"**
- Testing an API assignment
- Claude suggested edge cases, security tests, performance tests
- Faculty refined and added to autograder

**Win 3: "Claude helped me explain a complex topic better"**
- Topic: Database normalization
- Claude generated 3 different real-world examples
- Faculty picked the best one for lecture

### Medium-Term Wins (Month 1-2)

**Win 4: "Students are having better design conversations"**
- Students use Claude to explore architecture options
- Come to office hours with more sophisticated questions
- Design documents show deeper thinking

**Win 5: "Autograding is actually assessing understanding"**
- Claude API grades design documents
- Identifies when explanations are superficial
- Provides specific feedback to students
- Faculty spot-checks and agrees with assessments

### Long-Term Wins (Semester End)

**Win 6: "Students are more prepared for industry"**
- Exit surveys show confidence in AI-assisted development
- Students can articulate when to use AI vs. work independently
- Projects are more sophisticated than previous years

**Win 7: "I'm teaching more advanced content"**
- AI handles boilerplate, students focus on architecture
- Covered design patterns that usually get skipped
- Student projects demonstrate deeper understanding

---

## Addressing Faculty Concerns

### "I'm not technical enough to use Claude"
**Response:** Claude is a conversation tool, not a programming tool. If you can describe what you need in English, Claude can help. Faculty don't need to be Claude experts - start with simple tasks and build from there.

**Confidence builder:** Have faculty use Claude for non-technical tasks first:
- "Summarize this research paper"
- "Help me write an email to students about late policy"
- "Suggest discussion questions for this reading"

Once comfortable with conversation, move to technical tasks.

### "What if students know Claude better than I do?"
**Response:** Students probably will know different Claude techniques - that's fine! Faculty expertise is in CS concepts, pedagogy, and assessment design. Claude is just a tool.

**Reframe:** Faculty don't need to be the best Claude users - they need to guide students in using Claude appropriately for learning.

### "I'm worried I'll look foolish if Claude gives wrong answers"
**Response:** Claude makes mistakes - that's a teaching opportunity! Model critical thinking by verifying Claude's outputs.

**Practice:** Intentionally show students examples where Claude gets things wrong, demonstrate how to verify and correct.

### "This feels like cheating"
**Response:** Professionals use AI tools. Teaching students to use them effectively is preparing them for reality, not enabling cheating.

**Mindset shift:** We're teaching "programming with AI assistance" - a different but legitimate skill from "programming without any help."

### "I don't have time to learn a new tool"
**Response:** Time investment upfront saves time later. Claude can:
- Generate test cases in minutes (vs. hours manually)
- Draft rubrics (vs. starting from scratch)
- Create example code (vs. writing every example yourself)

**Start small:** Even 2-3 hours of initial learning yields time savings within weeks.

---

## Measuring Faculty Confidence Growth

**Pre-Workshop Survey:**
- How comfortable are you using AI tools? (1-5)
- Have you used Claude before? (Y/N)
- How confident are you in teaching with AI tools? (1-5)
- What concerns do you have?

**Mid-Semester Check:**
- How often do you use Claude for course prep? (Daily/Weekly/Monthly/Never)
- Name 2-3 ways Claude has helped you
- What challenges have you encountered?
- Confidence level now? (1-5)

**End-of-Semester Reflection:**
- Would you continue using Claude next semester? (Y/N)
- What was most valuable about Claude?
- What would you recommend to other faculty?
- Confidence level? (1-5)

**Target Metrics:**
- 80%+ of faculty report increased confidence by semester end
- 90%+ plan to continue using Claude
- Average confidence score increases from ~2.5 to ~4.0

---

## Success Indicators

**Faculty are confident when they:**
1. Use Claude regularly for course preparation (weekly or more)
2. Can articulate Claude's strengths and limitations
3. Troubleshoot student issues involving Claude
4. Design assignments that leverage Claude appropriately
5. Share tips and techniques with colleagues
6. Advocate for continued AI integration

**Most important:** Faculty view Claude as a helpful tool, not a threat or burden.

## Conclusion

This implementation plan provides a structured, realistic path to successfully integrating Claude AI into BYU Computer Science courses by Fall 2026. The phased approach with clear milestones, responsible parties, and risk mitigation strategies positions the department for a successful pilot that can expand to serve more students in subsequent semesters.

**Key to success:** Faculty preparation, enhanced autograding, clear policies, and continuous support throughout the Fall 2026 semester.

For questions or to discuss this plan, contact: [AI Integration Coordinator - TBD]
